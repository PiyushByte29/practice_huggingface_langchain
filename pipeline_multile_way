# First, make sure you have the necessary libraries installed:
# pip install transformers accelerate torch langchain_huggingface

from langchain_huggingface import HuggingFacePipeline
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from langchain.prompts import PromptTemplate
import torch

# --- 1. Using HuggingFacePipeline.from_model_id (the easy way) ---
# This is like buying the pre-assembled coffee machine.
# You tell LangChain the model ID and the task, and it sets up the pipeline for you.

llm_summarization = HuggingFacePipeline.from_model_id(
    model_id="facebook/bart-large-cnn",
    task="summarization",
    device=0
)

llm_refinement = HuggingFacePipeline.from_model_id(
    model_id="facebook/bart-large",
    task="summarization",
    device=0
)

qa_pipeline = pipeline("question-answering",
                       model="deepset/roberta-base-squad2", device=0)

summary_template = PromptTemplate.from_template(
    "Summarize the following text in a {length} way:\n\n{text}")

summarization_chain = summary_template | llm_summarization | llm_refinement

# Ask the user for the text they want summarized
text_to_summarize = input("\nEnter text to summarize:\n")

# Ask the user for desired summary length
length = input("\nEnter the length (short/medium/long): ")

summary = summarization_chain.invoke(
    {"text": text_to_summarize, "length": length})

# Print the generated summary
print("\nðŸ”¹ **Generated Summary:**")
print(summary)

# Interactive loop for asking questions about the summary
while True:
    # Ask user for a question
    question = input(
        "\nAsk a question about the summary (or type 'exit' to stop):\n")

    # Exit condition
    if question.lower() == "exit":
        break

    # Pass the question and summary as context into the QA pipeline
    qa_result = qa_pipeline(question=question, context=summary)

    # Display the answer extracted from the summary
    print("\nðŸ”¹ **Answer:**")
    print(qa_result["answer"])
